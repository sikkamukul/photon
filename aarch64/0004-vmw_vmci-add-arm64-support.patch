From b934169be7ff51e614e8e78b09a2313d64e32861 Mon Sep 17 00:00:00 2001
From: Cyprien Laplace <claplace@vmware.com>
Date: Fri, 29 Nov 2019 16:27:46 -0500
Subject: [PATCH 4/6] vmw_vmci: add arm64 support

VMCI is using PCI IO ports for datagram communication and shared
memory for queue pairs. Both need updates for arm64 support.

Replace inefficient one byte at a time arm64 implementations of
ioread8_rep() and iowrite8_rep() with a single VM exit hypercall.

Add virtualization specific barriers for queue pair (de)queueing,
following the core-api/circular-buffers.rst example, that will
map to actual memory barrier instructions on arm64.

Disable driver if 16KB or 64KB pages are selected as the VMware
hypervisor assumes 4KB page size at the moment.

Tested with open-vm-tools, vmw_balloon and vsock mixedTest on arm64.

Signed-off-by: Cyprien Laplace <claplace@vmware.com>
Signed-off-by: Keerthana K <keerthanak@vmware.com>
---
 arch/arm64/include/asm/vmware.h    | 70 ++++++++++++++++++++++++++++++
 arch/x86/include/asm/vmware.h      |  6 +++
 drivers/misc/vmw_vmci/Kconfig      |  3 +-
 drivers/misc/vmw_vmci/vmci_guest.c | 20 +++++----
 include/linux/vmw_vmci_defs.h      | 13 +++++-
 5 files changed, 100 insertions(+), 12 deletions(-)

diff --git a/arch/arm64/include/asm/vmware.h b/arch/arm64/include/asm/vmware.h
index eef4e2688..24946f144 100644
--- a/arch/arm64/include/asm/vmware.h
+++ b/arch/arm64/include/asm/vmware.h
@@ -49,4 +49,74 @@
 #define VMWARE_HYPERVISOR_PORT_HB 0x5659
 #define VMWARE_HYPERVISOR_MAGIC 0x564D5868
 
+/*
+ * Efficient I/O port accesses for vmware virtual machines. Execute
+ * a single VM exit instead of one per byte. This feature is limitied
+ * to the first 64K I/O ports.
+ */
+
+static inline void __vmw_rep_outsb(u16 port, const void *buffer,
+				   unsigned int count)
+{
+	register u64 x2 asm("x2") = count;
+	register u32 w3 asm("w3") = port;
+	register u64 x4 asm("x4") = (u64)buffer;
+	register u32 w7 asm("w7") =   X86_IO_W7_STR
+				    | X86_IO_W7_WITH;
+
+	asm volatile(
+		"hvc %4" :
+		"+r" (x2),
+		"+r" (x4) :
+		"r" (w3),
+		"r" (w7),
+		"i" (X86_IO_MAGIC)
+	);
+}
+
+static inline void vmw_iowrite8_rep(void __iomem *addr,
+				    const void *buffer,
+				    unsigned int count)
+{
+	unsigned long port = (unsigned long)addr - PCI_IO_START;
+
+	if (WARN_ONCE((u16)port != port, "Invalid I/O address\n"))
+		return;
+
+	__vmw_rep_outsb(port, buffer, count);
+}
+
+static inline void __vmw_rep_insb(u16 port, void *buffer,
+				  unsigned int count)
+{
+	register u64 x2 asm("x2") = count;
+	register u32 w3 asm("w3") = port;
+	register u64 x5 asm("x5") = (u64)buffer;
+	register u32 w7 asm("w7") =   X86_IO_W7_STR
+				    | X86_IO_W7_WITH
+				    | X86_IO_W7_DIR;
+
+	asm volatile(
+		"hvc %4" :
+		"+r" (x2),
+		"+r" (x5) :
+		"r" (w3),
+		"r" (w7),
+		"i" (X86_IO_MAGIC) :
+		"memory"
+	);
+}
+
+static inline void vmw_ioread8_rep(const void __iomem *addr,
+				   void *buffer,
+				   unsigned int count)
+{
+	unsigned long port = (unsigned long)addr - PCI_IO_START;
+
+	if (WARN_ONCE((u16)port != port, "Invalid I/O address\n"))
+		return;
+
+	__vmw_rep_insb(port, buffer, count);
+}
+
 #endif /* _ASM_ARM64_VMWARE_H */
diff --git a/arch/x86/include/asm/vmware.h b/arch/x86/include/asm/vmware.h
index e88c50442..0a537d62f 100644
--- a/arch/x86/include/asm/vmware.h
+++ b/arch/x86/include/asm/vmware.h
@@ -7,6 +7,12 @@
 #include <asm/alternative.h>
 #include <linux/stringify.h>
 
+/*
+ * Standard I/O port access for virtual devices.
+ */
+#define vmw_iowrite8_rep iowrite8_rep
+#define vmw_ioread8_rep ioread8_rep
+
 #define CPUID_VMWARE_INFO_LEAF               0x40000000
 #define CPUID_VMWARE_FEATURES_LEAF           0x40000010
 #define CPUID_VMWARE_FEATURES_ECX_VMMCALL    BIT(0)
diff --git a/drivers/misc/vmw_vmci/Kconfig b/drivers/misc/vmw_vmci/Kconfig
index 605794aad..566a6e751 100644
--- a/drivers/misc/vmw_vmci/Kconfig
+++ b/drivers/misc/vmw_vmci/Kconfig
@@ -5,7 +5,8 @@
 
 config VMWARE_VMCI
 	tristate "VMware VMCI Driver"
-	depends on X86 && PCI
+	depends on (X86 || ARM64) && PCI
+	depends on !ARM64_16K_PAGES && !ARM64_64K_PAGES && !CPU_BIG_ENDIAN
 	help
 	  This is VMware's Virtual Machine Communication Interface.  It enables
 	  high-speed communication between host and guest in a virtual
diff --git a/drivers/misc/vmw_vmci/vmci_guest.c b/drivers/misc/vmw_vmci/vmci_guest.c
index cc8eeb361..1d32759b3 100644
--- a/drivers/misc/vmw_vmci/vmci_guest.c
+++ b/drivers/misc/vmw_vmci/vmci_guest.c
@@ -21,6 +21,8 @@
 #include <linux/io.h>
 #include <linux/vmalloc.h>
 
+#include <asm/vmware.h>
+
 #include "vmci_datagram.h"
 #include "vmci_doorbell.h"
 #include "vmci_context.h"
@@ -114,8 +116,8 @@ int vmci_send_datagram(struct vmci_datagram *dg)
 	spin_lock_irqsave(&vmci_dev_spinlock, flags);
 
 	if (vmci_dev_g) {
-		iowrite8_rep(vmci_dev_g->iobase + VMCI_DATA_OUT_ADDR,
-			     dg, VMCI_DG_SIZE(dg));
+		vmw_iowrite8_rep(vmci_dev_g->iobase + VMCI_DATA_OUT_ADDR,
+				 dg, VMCI_DG_SIZE(dg));
 		result = ioread32(vmci_dev_g->iobase + VMCI_RESULT_LOW_ADDR);
 	} else {
 		result = VMCI_ERROR_UNAVAILABLE;
@@ -216,8 +218,8 @@ static void vmci_dispatch_dgs(unsigned long data)
 
 	BUILD_BUG_ON(VMCI_MAX_DG_SIZE < PAGE_SIZE);
 
-	ioread8_rep(vmci_dev->iobase + VMCI_DATA_IN_ADDR,
-		    vmci_dev->data_buffer, current_dg_in_buffer_size);
+	vmw_ioread8_rep(vmci_dev->iobase + VMCI_DATA_IN_ADDR,
+			vmci_dev->data_buffer, current_dg_in_buffer_size);
 	dg = (struct vmci_datagram *)dg_in_buffer;
 	remaining_bytes = current_dg_in_buffer_size;
 
@@ -277,7 +279,7 @@ static void vmci_dispatch_dgs(unsigned long data)
 					current_dg_in_buffer_size =
 					    dg_in_buffer_size;
 
-				ioread8_rep(vmci_dev->iobase +
+				vmw_ioread8_rep(vmci_dev->iobase +
 						VMCI_DATA_IN_ADDR,
 					vmci_dev->data_buffer +
 						remaining_bytes,
@@ -319,7 +321,7 @@ static void vmci_dispatch_dgs(unsigned long data)
 				current_dg_in_buffer_size = dg_in_buffer_size;
 
 			for (;;) {
-				ioread8_rep(vmci_dev->iobase +
+				vmw_ioread8_rep(vmci_dev->iobase +
 						VMCI_DATA_IN_ADDR,
 					vmci_dev->data_buffer,
 					current_dg_in_buffer_size);
@@ -339,9 +341,9 @@ static void vmci_dispatch_dgs(unsigned long data)
 		if (remaining_bytes < VMCI_DG_HEADERSIZE) {
 			/* Get the next batch of datagrams. */
 
-			ioread8_rep(vmci_dev->iobase + VMCI_DATA_IN_ADDR,
-				    vmci_dev->data_buffer,
-				    current_dg_in_buffer_size);
+			vmw_ioread8_rep(vmci_dev->iobase + VMCI_DATA_IN_ADDR,
+					vmci_dev->data_buffer,
+					current_dg_in_buffer_size);
 			dg = (struct vmci_datagram *)dg_in_buffer;
 			remaining_bytes = current_dg_in_buffer_size;
 		}
diff --git a/include/linux/vmw_vmci_defs.h b/include/linux/vmw_vmci_defs.h
index be0afe6f3..4ef0fb60c 100644
--- a/include/linux/vmw_vmci_defs.h
+++ b/include/linux/vmw_vmci_defs.h
@@ -743,7 +743,12 @@ static inline void *vmci_event_data_payload(struct vmci_event_data *ev_data)
  */
 static inline u64 vmci_q_read_pointer(u64 *var)
 {
-	return READ_ONCE(*(unsigned long *)var);
+	/*
+	 * The virt_load_acquire() barrier ensures that all memory
+	 * accesses after the load cannot be speculated and executed
+	 * before it.
+	 */
+	return virt_load_acquire((unsigned long *)var);
 }
 
 /*
@@ -755,7 +760,11 @@ static inline u64 vmci_q_read_pointer(u64 *var)
 static inline void vmci_q_set_pointer(u64 *var, u64 new_val)
 {
 	/* XXX buggered on big-endian */
-	WRITE_ONCE(*(unsigned long *)var, (unsigned long)new_val);
+	/*
+	 * The virt_store_release() ensures that all memory operations
+	 * done before the store will appear to happen before it.
+	 */
+	virt_store_release((unsigned long *)var, (unsigned long)new_val);
 }
 
 /*
-- 
2.28.0

